# Market Oracle v2 - ML Project

# **THE 4-WEEK DAY-BY-DAY ROADMAP**

---

# **ðŸŸ¦ WEEK 1 â€” ML FOUNDATIONS + PHASE 1 CLASSIFIER**

**Goal:**

âž¡ Learn ML fundamentals

âž¡ Build feature engineering pipeline

âž¡ Build a walk-forward validated logistic regression & random forest classifier

---

## **DAY 1 â€” Python + pandas Refresher (2â€“3 hrs)**

### Learn:

- pandas indexing
- rolling windows
- merging
- matplotlib basics

### Resources:

- Corey Schafer pandas playlist (best intro)
- aiml.com â†’ â€œPython for MLâ€ basics

### Code Tasks:

- Load a CSV in pandas
- Plot close price
- Compute log returns manually

---

## **DAY 2 â€” ML Fundamentals (4 hrs)**

### Learn:

- What is supervised learning
- Classification vs regression
- Overfitting
- Train/test splits
- Walk-forward validation concept

### Resources:

- aiml.com â†’ ML Fundamentals
- D2L Section 1.1 + 2.1

### Code Tasks:

- Implement simple train/test split
- Train logistic regression using scikit-learn

---

## **DAY 3 â€” Downloading Stock Data + Cleaning (3 hrs)**

### Learn:

- Time-series indexing
- Handling NA
- Visualizing trends

### Resources:

- yfinance documentation

### Code Tasks:

- Build a script `data_loader.py`:
    - Download ticker data
    - Save CSV to `/data/raw/`
    - Clean NA values
    - Compute log returns

---

## **DAY 4 â€” Technical Indicators (3â€“4 hrs)**

### Learn:

- RSI
- MACD
- SMA
- Rolling volatility

### Resources:

- YouTube: â€œCompute RSI MACD in Pythonâ€
- aiml.com â†’ Feature Engineering

### Code Tasks:

Implement in `indicators.py`:

- RSI
- MACD
- SMA 50
- SMA 200
- Rolling std

---

## **DAY 5 â€” Walk-Forward Validation (4 hrs)**

### Learn:

- Why random splits are invalid for time-series
- Expanding window approach

### Resources:

- â€œMachineLearningMastery Walk Forward Validationâ€ article

### Code Tasks:

Implement `walk_forward.py`:

- Split data into folds
- For each fold: train on earlier data, test on next segment

---

## **DAY 6 â€” Phase 1 Model Training (4â€“5 hrs)**

### Learn:

- Logistic Regression details
- Random Forest (feature importance)

### Code Tasks:

Notebook `03_phase1_classifier.ipynb`:

- Train both models
- Evaluate accuracy, precision
- Plot feature importance

---

## **DAY 7 â€” Review + Documentation Day**

### Tasks:

- Clean code
- Document feature pipeline
- Make README section for Phase 1
- Push Week 1 progress to GitHub

---

# ðŸŸ¦ WEEK 2 â€” DEEP LEARNING FOUNDATIONS + LSTM MODEL (PHASE 2)

**Goal:**

âž¡ Learn deep learning basics

âž¡ Understand LSTM fully

âž¡ Build LSTM for log-return forecasting

âž¡ Use walk-forward again

---

## **DAY 8 â€” Neural Network Basics (3 hrs)**

### Learn:

- What are layers
- Weights, biases
- Activation functions
- Loss functions
- Gradient descent

### Resources:

- aiml.com â†’ Deep Learning Basics
- D2L 3.6 + 4.1

---

## **DAY 9 â€” RNNs & LSTMs Theory (4 hrs)**

### Learn:

- Why RNNs fail (vanishing gradient)
- How LSTM solves it
- Forget gate
- Cell state

### Resources:

- D2L 8.1, 8.2, 9.1
- StatQuest LSTM video

---

## **DAY 10 â€” TensorFlow Fundamentals (3 hrs)**

### Learn:

- Keras Sequential API
- Layers
- Loss + optimizer
- Training loop

### Resource:

- TensorFlow Beginner Tutorials

### Code Tasks:

- Make a toy neural network (MNIST)
- Train for 5 epochs

---

## **DAY 11 â€” Windowing Time Series (3 hrs)**

### Learn:

- Sliding windows
- Supervised dataset creation

### Code Tasks:

Implement function:

```
create_windows(data, window=30)

```

Outputs:

- X : (samples, 30, features)
- y : next-day log return

---

## **DAY 12 â€” Build LSTM Model (4â€“5 hrs)**

### Code Tasks:

Notebook `04_lstm_model.ipynb`:

- LSTM(64 units)
- Dropout
- Dense(1)
- Loss: MSE
- Optimizer: Adam

---

## **DAY 13 â€” Walk-Forward Training + Metrics (3 hrs)**

### Code Tasks:

- Apply walk-forward to LSTM
- Save RMSE results
- Compute â€œdirection accuracyâ€ (sign match)

### Visualization:

- Plot predicted vs actual returns

---

## **DAY 14 â€” Review + Refactor**

- Clean code
- Add comments
- Update README
- Push to GitHub

---

# ðŸŸ¦ WEEK 3 â€” NLP + SENTIMENT + ATTENTION (PHASE 3)

**Goal:**

âž¡ Learn sentiment analysis

âž¡ Integrate sentiment with price features

âž¡ Understand attention

âž¡ Build multi-input LSTM+Attention model

---

## **DAY 15 â€” NLP Basics (3 hrs)**

### Learn:

- Tokenization
- Stopwords
- Bag of words
- Why simple sentiment models work

### Resources:

- aiml.com NLP Intro
- D2L NLP Intro

---

## **DAY 16 â€” VADER Sentiment (2 hrs) + News Scraping (2 hrs)**

### Learn:

- Using NLTK VADER
- Aggregating daily sentiment
- Avoiding leakage: use sentiment(t) â†’ predict(t+1)

### Code Tasks:

`sentiment.py`:

- Fetch news (NewsAPI)
- Compute VADER score
- Merge with price dataset

---

## **DAY 17 â€” Attention Mechanism Theory (3 hrs)**

### Learn:

- Why attention helps
- Key / value / query concept

### Resources:

- D2L 9.4 Attention
- YouTube: â€œAttention Explained Simplyâ€

---

## **DAY 18 â€” Implement Attention Layer (3â€“4 hrs)**

### Code Tasks:

Custom Keras layer:

```
score = tanh(W1*h + b1)
attention_weights = softmax(score)
context = sum(attention_weights * h)

```

Add before Dense layer.

---

## **DAY 19 â€” Train Attention LSTM Model (4 hrs)**

### Code Tasks:

`05_attention_model.ipynb`:

- Train walk-forward
- Compare RMSE
- Compare direction accuracy
- Plot attention weights

---

## **DAY 20 â€” Evaluate Sentiment Impact (3 hrs)**

### Analysis Tasks:

- Compare LSTM vs LSTM+Sentiment
- Compare LSTM vs Attention model
- Save results table

---

## **DAY 21 â€” Documentation Day**

- Clean everything
- Update README Phase 3 section
- Push Week 3 to GitHub

---

# ðŸŸ¦ WEEK 4 â€” BACKTESTING ENGINE + FINALIZATION

**Goal:**

âž¡ Build a simple trading engine

âž¡ Evaluate Sharpe, drawdown, CAGR

âž¡ Produce charts

âž¡ Finalize GitHub + Report

---

## **DAY 22 â€” Backtesting Basics (3 hrs)**

### Learn:

- Buy/sell strategy
- Drawdown calculation
- Sharpe ratio

### Resource:

- QuantInsti YouTube basics on backtesting

---

## **DAY 23 â€” Implement Backtesting Engine (4 hrs)**

### Code Tasks:

`backtester.py`:

Inputs: predicted returns

Rules:

```
IF predicted return > 0 â†’ BUY
ELSE â†’ STAY IN CASH

```

Outputs:

- Equity curve
- CAGR
- Sharpe (mean/vol)
- Max drawdown

---

## **DAY 24 â€” Full Pipeline Integration (4 hrs)**

Script `main.py`:

- Load raw data
- Compute indicators
- Add sentiment
- Create windows
- Run model
- Backtest strategy
- Save results

---

## **DAY 25 â€” Visualization + Metrics Summary (3 hrs)**

Charts to generate:

- Equity curve
- Daily returns
- Attention heatmaps
- Feature importance (Phase 1)

---

## **DAY 26 â€” Write Final Report (4 hrs)**

Sections:

1. Abstract
2. Problem Statement
3. Data
4. Methodology
5. Models
6. Results
7. Backtest performance
8. Limitations
9. Future Work

---

## **DAY 27 â€” Final GitHub Polish**

README MUST INCLUDE:

- Architecture diagram
- Results summary
- Installation instructions
- Usage guide

---

## **DAY 28 â€” Buffer Day (Fix bugs + Viva prep)**

Prepare answers for:

- Why walk-forward validation?
- Why not predict price?
- Why LSTM?
- Why attention?
- How did sentiment help?

---

# ðŸŸ© **OUTPUT BY END OF 4 WEEKS**

You will have:

âœ” Fully working ML + DL + NLP + Quant project

âœ” Professional GitHub repo

âœ” Research-style PDF report

âœ” Beautiful plots

âœ” Production-ready code structure

âœ” Resume-grade achievement

This is **strong enough to be your capstone project or internship centerpiece**.